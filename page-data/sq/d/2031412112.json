{"data":{"featured":{"edges":[{"node":{"frontmatter":{"title":"AI-Powered Video Content Analyzer","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABQElEQVQoz41S21LDIBDNs1KugRCatJnGmlHr9dH//6/jLA2UxtTx4Qwse/bsjWojHQhMNeDGg3EHcxwQvp9g38YI935BtD8foPZd5G5UE8GkA9cNKm1aDMMj+sMEFwZw7TNpo89gxT1D/UYUVHaLw3TCOJ0g64B7YXPGBHqnROVbQuruTlhw1aAiMlXJpVtk83MbHtJuIUy76iMxJR1e9kcIG1AR2VAFM2EtiMToTH6yyZeScOXgXQdhPCpWVHZL8GquC18aT265FLolqF0X5/iXYF4KUw5CtqhFD6t2qGUfYVQPJbawao/WjWjMAC27GCh1gBF9PGOSeUGXCldaFtRqUWG5FEm8RYVZMBrCgq1sOQlSu/8WJGLtdzmoFGZyFij+YXwv5nZGIajqgOn5A0+vXwj9ePWxlx+8tNfuJPgDmlYmMNpuZZUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/85391/timeline.png","srcSet":"/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/cebcc/timeline.png 175w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/b3b96/timeline.png 350w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/85391/timeline.png 700w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/88fa4/timeline.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/9aa63/timeline.avif 175w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/f847f/timeline.avif 350w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/d6d4f/timeline.avif 700w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/04d38/timeline.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/240e7/timeline.webp 175w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/5f909/timeline.webp 350w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/4d9a8/timeline.webp 700w,\n/Portfolio/static/0cbcfec163de066a3bfd1fb366db7fe4/bcfc5/timeline.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":435}}},"tech":["Computer Vision","NLP","OpenCV","PyTorch","Python"],"github":"https://github.com/sej07/VideoContentAnalyzer","external":"https://huggingface.co/spaces/Sej7/Video-Content-Analyzer","cta":null},"html":"<p>Built an end-to-end multimodal AI pipeline integrating YOLOv8 object detection (82% mAP), Whisper transcription (&#x3C;10% WER), and CLIP scene understanding (84% accuracy) to reduce 10-minute video analysis from 30+ minutes to 2.5 minutes</p>\n<p>Engineered FastAPI backend with async job processing and RESTful endpoints to enable concurrent video uploads with real-time status tracking and sub-500ms query response times</p>\n<p>Deployed production system via Docker to Hugging Face Spaces handling 500MB videos to achieve 86% object tracking ID consistency across occlusions and &#x3C;500ms API response times for status queries</p>"}},{"node":{"frontmatter":{"title":"Research Paper Classifier","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB7ElEQVQoz2VS2W7cMAz0bta3LV+SLFvysbb3yKZtUqBoiwL9/8+agnQQIOgDddDUcDhjT7c9tLEoaw3V9pC6R6MM71EiOOK04D0vGq5L8gphnCMTNZKs3Pe8wm19wAujDF3rcF6ucOOCcd4wny98npcLOjvxgyDKIFsLN17QuxWNdqi1gzIj7LghySp4Bx8eLXFaws03FHULUSqkeY1TkCCIc85RcSoaTPMd+mVF+XNCrTuuS/OKmRPjw1MIzzsGEIXEt7df6MYLtD2jVBaVcpDdzHeKfrpy2PWGfrtiPG94fnzlyYZp4QkYkBbSZ9meuRt9oIizEqaf0PYTGm2RiZ1FLhpIZWH6EcpYzpHGNNEnQDLBD1MuIEDKVY1hE+hO37hRWkCUElnRsCSs2zFgsOMp2gGpkEwg8f0o27sSQzuhkuY9J7iOmNaqYwKk3ZMfMzsCY4Z0oISbVvRuRhDlzIaYdHaG7gbU0kBUihulomZAcrmUPQNSfAAywzCFVIY7EtjeNeWRyOG8kCwBsaQ7jSvbAZV2/wPSwQ8STNMC3Voehf49bRwzJhlIo4+H7yM27YDGzAhigVOw5z8AqfDx5Q2///zF6/cfWLYblu2O6/2FwcmEolIcZNIpTLEMDq/XFYMsIRsNPylxOAb4B4bMB8Ielg15AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/1dc65/demo.png","srcSet":"/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/9a130/demo.png 175w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/47c72/demo.png 350w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/1dc65/demo.png 700w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/4aa24/demo.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/dae43/demo.avif 175w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/69c10/demo.avif 350w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/ebe22/demo.avif 700w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/e0c37/demo.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/5d873/demo.webp 175w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/853c6/demo.webp 350w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/978b5/demo.webp 700w,\n/Portfolio/static/9b5b6355cdca4c0df449fa7446d4d691/1b4eb/demo.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":392}}},"tech":["PyTorch","BERT","Transformers","Python"],"github":"https://github.com/sej07/Research-Paper-Classification-Fine-Tuning-BERT-","external":null,"cta":null},"html":"<p>Designed an end-to-end document classification system by fine-tuning BERT-base on 110M parameters for multi-class classification across 11 academic categories, with a modular, production-oriented codebase and validation pipeline</p>\n<p>Conducted detailed error analysis using confusion matrices, identifying conceptual overlap between closely related categories and documenting insights to guide future dataset refinement and model improvements</p>"}},{"node":{"frontmatter":{"title":"Neural Machine Translation System","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB3UlEQVQoz01S6XriMAxMKeQ+nMu5EyCBlEI52t2+/5vNfiMW2h+SLVueGckydFFDlw1UopEXDTJdI8lKuL6C7YZwvEiM+yBKJc8NYlhOAD9MJE/WIMY8vsOwbB9l0WK73WPY7NB0G6y3ezGVFk9Q0/aR6QZtv0Pdjkh1i0S3yMseTT/B9WMYLysYdI6n0G1mREmBQGXwwgRLy4PpBIj+K/JVhmF9gD6OUN8DEl3BDxI4VBilQvjyasEwFiZCleN0/kbZTdD1BiprEOctsmqNpOgR61bOq25CM86oph26YYv57Yhpf8A4vUklxDKIansR1uMs/SATjcxF1csdY/ZsZXlCnmY1tuOM6+dfnC9fOF+/EKrsXjIBLSeUx78BycgP4PoA5OoFibSFgMfTFbv5iM24/ymZjkHbj4ji/PmYfSUJme9nd2DGaV5DV53kU/XSdKXcxdKGQceDbhhF0evKkZjKdNkJOGM+5B1Hh78dRBlsNxIgiiKOKOSGyUXVSckEEIW+EiUcGcYk4EqgOC1lXgn+EPAEpBNAXcFxw6eapXkvhePzKOtunijjDKqsFsDF0vkBZBJ7M+4OuNz+4HS+4fhxxfvpInb6uMnAqzhHnBbSFuYX9YCs6ITM/EX6D7mpB8qvvESZAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/45528/demo.png","srcSet":"/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/68202/demo.png 175w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/aef44/demo.png 350w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/45528/demo.png 700w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/420ce/demo.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/1934d/demo.avif 175w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/4564e/demo.avif 350w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/0d964/demo.avif 700w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/971ed/demo.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/745fa/demo.webp 175w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/37527/demo.webp 350w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/fee38/demo.webp 700w,\n/Portfolio/static/024126fc1173dfd05d11999bc74d29e8/8e184/demo.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":398}}},"tech":["TensorFlow","Keras","LSTM","Attention"],"github":null,"external":"https://github.com/sej07/Reimplementation-of-Seq2Seq-paper","cta":"https://github.com/sej07/Reimplementation-of-Seq2Seq-paper"},"html":"<p>Reimplemented encoderâ€“decoder training loop for Seq2Seq models with attention using TensorFlow, training on 40k sentence pairs from WMT14 dataset while analyzing common failure modes such as exposure bias and long-sequence degradation.</p>\n<p>Designed 4-layer LSTM encoder-decoder architecture using Bahdanau attention mechanism to handle variable-length sequences up to 50 tokens and 512-dimensional hidden state representations</p>"}}]}}}